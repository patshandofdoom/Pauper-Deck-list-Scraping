{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patshandofdoom/Pauper-Deck-list-Scraping/blob/main/Pauper_Deck_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLbUOVveT88q"
      },
      "source": [
        "Features to add:\n",
        "*  Review script and see if any optimizations can be made\n",
        "* Evaluate downloading other decklists from Wizards\n",
        "* evaluate downloading other Decklists from"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gathering info"
      ],
      "metadata": {
        "id": "NmC014PMHSTk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD-GHKQAQmeb"
      },
      "source": [
        "First we need to install all the functions needed to make this scraper work. Beautiful soup is key here as it allows us to search the HTML for our"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7LvjXgzT33-l"
      },
      "outputs": [],
      "source": [
        "from html import escape\n",
        "from html import unescape\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "import json\n",
        "from datetime import datetime, timedelta, date\n",
        "from dateutil import parser\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Go8djzl-Udn5"
      },
      "source": [
        "Next we define all the functions that handle the information. This section could use some work, but it functions right now sooooo...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU1v3Jqh4yhY"
      },
      "outputs": [],
      "source": [
        "#Defines a function that returns event ditionary, which is the building block of our program\n",
        "\n",
        "def event_define(deck_link_tail):\n",
        "  #Build the link for the event\n",
        "  deck_link = 'https://www.mtggoldfish.com'+deck_link_tail+'#paper'\n",
        "\n",
        "  #Get the gonctent from the page and parse the HTML\n",
        "  deck_page = requests.get(deck_link, headers={\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"})\n",
        "  deck_page_soup = BeautifulSoup(deck_page.content, \"html.parser\")\n",
        "\n",
        "  #Find the event name within the soup\n",
        "  event_name = deck_page_soup.find('p', class_='deck-container-information').find_next('a').get_text()\n",
        "\n",
        "  #Parse the event date from the event name using Regex\n",
        "  event_date = re.sub('\\s','',re.sub('[A-z]+','',event_name))\n",
        "\n",
        "  #Build the event link using components from the HTML\n",
        "  event_link = 'https://www.mtggoldfish.com'+deck_page_soup.find('p', class_='deck-container-information').find_next('a').get('href')\n",
        "\n",
        "  #Create a dictionary for the event and return it\n",
        "  event= {\n",
        "        \"Event Name\":event_name,\n",
        "        \"Event Date\":event_date,\n",
        "        \"Event Link\":event_link,\n",
        "        \"Decks\":{}\n",
        "        }\n",
        "\n",
        "  return event\n",
        "##################################################\n",
        "#Defines a function that returns the deck soup of the deck after grabbing it with beautifulsoup\n",
        "def get_html(index, deck_link_tail):\n",
        "  #Build the deck link\n",
        "  deck_link = 'https://www.mtggoldfish.com'+deck_link_tail+'#paper'\n",
        "  #send the request with the proper headers to get a response\n",
        "  deck_page = requests.get(deck_link, headers={\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"})\n",
        "  #Convert the response into a searchable object with beautifulsoup\n",
        "  deck_page_soup = BeautifulSoup(deck_page.content, \"html.parser\")\n",
        "\n",
        "  #return the deck page soup so that we dont need to call the HTML again in the next function\n",
        "  return(deck_page_soup)\n",
        "\n",
        "##################################################\n",
        "#Create a deck info function in order to simplify the 'for loop' bullshit and make it more clear\n",
        "\n",
        "def deck_info_function(deck_page_soup, deck_link_tail, event,index):\n",
        "  #remove spaces and linebreaks\n",
        "  deck_link = 'https://www.mtggoldfish.com'+deck_link_tail+'#paper'\n",
        "  deck_name = re.sub('\\n',\"\",deck_page_soup.find('h1', class_='title').contents[0])\n",
        "  pilot = re.sub(\"^by\\s\",\"\",deck_page_soup.find(\"span\", class_=\"author\").get_text())\n",
        "  format = re.sub(\"[A-z]*:\\s\",\"\",re.sub(\"\\n\",\"\",deck_page_soup.find('p', class_='deck-container-information').contents[0]))\n",
        "\n",
        "  archetype_container = deck_page_soup.find('p', class_='deck-container-information').find_all('a')\n",
        "  #not all decks have an archetype designated so we need to check if it exists before we try and access it\n",
        "  if((archetype_container is not None) and (len(archetype_container)>=3)):\n",
        "    archetype = archetype_container[2].get_text()\n",
        "    archetype_link = archetype_container[2].get(\"href\")\n",
        "  else:\n",
        "    archetype = \"Unlisted\"\n",
        "    archetype_link = \"Unlisted\"\n",
        "\n",
        "  event[\"Decks\"].update({\"Deck \"+str(index):\n",
        "    {\n",
        "    \"DeckName\":deck_name,\n",
        "    \"Pilot\":pilot,\n",
        "    \"DeckLink\":deck_link,\n",
        "    \"Format\":format,\n",
        "    \"Archetype\":archetype,\n",
        "    \"Archetype Link\":\"https://www.mtggoldfish.com\"+archetype_link,\n",
        "    \"Mainboard\":{},\n",
        "    \"Sideboard\":{}\n",
        "    }})\n",
        "  return event\n",
        "\n",
        "##################################################\n",
        "def add_card(this_object, card_name,card_quantity,main_side):\n",
        "  if main_side == \"main\":\n",
        "    this_object[\"Mainboard\"].update({card_name:card_quantity})\n",
        "  else:\n",
        "    this_object[\"Sideboard\"].update({card_name:card_quantity})\n",
        "\n",
        "##################################################\n",
        "#Defines a function that returns the deck soup of the deck after grabbing it with beautifulsoup\n",
        "\n",
        "def get_html(index, deck_link_tail):\n",
        "  #Build the deck link\n",
        "  deck_link = 'https://www.mtggoldfish.com'+deck_link_tail+'#paper'\n",
        "  #send the request with the proper headers to get a response\n",
        "  deck_page = requests.get(deck_link, headers={\"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246\"})\n",
        "  #Convert the response into a searchable object with beautifulsoup\n",
        "  deck_page_soup = BeautifulSoup(deck_page.content, \"html.parser\")\n",
        "\n",
        "  #return the deck page soup so that we dont need to call the HTML again in the next function\n",
        "  return(deck_page_soup)\n",
        "\n",
        "##################################################\n",
        "#Create a deck info function in order to simplify the 'for loop' bullshit and make it more clear\n",
        "\n",
        "def deck_info_function(deck_page_soup, deck_link_tail, event,index):\n",
        "  #remove spaces and linebreaks\n",
        "  deck_link = 'https://www.mtggoldfish.com'+deck_link_tail+'#paper'\n",
        "  deck_name = re.sub('\\n',\"\",deck_page_soup.find('h1', class_='title').contents[0])\n",
        "  pilot = re.sub(\"^by\\s\",\"\",deck_page_soup.find(\"span\", class_=\"author\").get_text())\n",
        "  format = re.sub(\"[A-z]*:\\s\",\"\",re.sub(\"\\n\",\"\",deck_page_soup.find('p', class_='deck-container-information').contents[0]))\n",
        "\n",
        "  archetype_container = deck_page_soup.find('p', class_='deck-container-information').find_all('a')\n",
        "  #not all decks have an archetype designated so we need to check if it exists before we try and access it\n",
        "  if((archetype_container is not None) and (len(archetype_container)>=3)):\n",
        "    archetype = archetype_container[2].get_text()\n",
        "    archetype_link = archetype_container[2].get(\"href\")\n",
        "  else:\n",
        "    archetype = \"Unlisted\"\n",
        "    archetype_link = \"Unlisted\"\n",
        "\n",
        "  event[\"Decks\"].update({\"Deck \"+str(index):\n",
        "    {\n",
        "    \"DeckName\":deck_name,\n",
        "    \"Pilot\":pilot,\n",
        "    \"DeckLink\":deck_link,\n",
        "    \"Format\":format,\n",
        "    \"Archetype\":archetype,\n",
        "    \"Archetype Link\":\"https://www.mtggoldfish.com\"+archetype_link,\n",
        "    \"Mainboard\":{},\n",
        "    \"Sideboard\":{}\n",
        "    }})\n",
        "  return event\n",
        "\n",
        "##################################################\n",
        "\n",
        "def add_card(this_object, card_name,card_quantity,main_side):\n",
        "  if main_side == \"main\":\n",
        "    this_object[\"Mainboard\"].update({card_name:card_quantity})\n",
        "  else:\n",
        "    this_object[\"Sideboard\"].update({card_name:card_quantity})\n",
        "\n",
        "##################################################\n",
        "def decklist_add(deck_html,index, event):\n",
        "  #Grab all card names and quantities from the decklist\n",
        "  deck_table = deck_html.find(\"table\",class_=\"deck-view-deck-table\")\n",
        "  deck_rows = deck_table.find_all(\"tr\")\n",
        "  #Set a switch that will let the program know where to store the card, either main-deck or sideboard\n",
        "  main_side_switch = 'main'\n",
        "\n",
        "  #For each of the rows, get the card name, card quantity, then add it to the deck\n",
        "  for each_row in deck_rows:\n",
        "    if each_row.find(\"th\") is None:\n",
        "      this_card_name = each_row.find_next(\"span\",class_=\"card_id card_name\").get_text()\n",
        "      this_card_quantity = re.sub(\"\\n\",\"\",each_row.find_next(\"td\",class_=\"text-right\").get_text())\n",
        "      add_card(event[\"Decks\"][\"Deck \"+str(index)],this_card_name,this_card_quantity,main_side_switch)\n",
        "    #If one of hte rows contains the sideboard header, change the switch so that the cards are added to the sideboard\n",
        "    else:\n",
        "      header_value = each_row.find(\"th\").get_text()\n",
        "      if re.search('Sideboard',header_value):\n",
        "        main_side_switch = 'side'\n",
        "\n",
        "##################################################\n",
        "\n",
        "def compile_event_info(link_to_event):\n",
        "  #Script commands actually start here, We start by grabbing the pauper league overview page\n",
        "  page = requests.get(link_to_event)\n",
        "  page_soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "  #Get each <tr> seperately into an array\n",
        "  #Divide all the <td>'s into arrays, grab the href from the <a> object from each of the second TD's\n",
        "  #This should give the link to each deck: \"https://www.mtggoldfish.com\" + href\n",
        "\n",
        "\n",
        "  deck_links = []\n",
        "  rows_for_tourney = page_soup.find(\"table\",class_=\"table-tournament\").find_all(\"tr\")\n",
        "\n",
        "  for each_row in rows_for_tourney:\n",
        "    table_cells = each_row.find_all(\"td\")\n",
        "    if len(table_cells) > 2:\n",
        "      deck_links.append(table_cells[1].find_next('a').get('href'))\n",
        "\n",
        "  #Need to establish a list of decklists for each event where we can push the deck info\n",
        "  decklists_for_event = []\n",
        "  event = event_define(deck_links[0])\n",
        "\n",
        "  for index, each_deck_link in enumerate(deck_links):\n",
        "    deck_html = get_html(index, each_deck_link)\n",
        "    event = deck_info_function(deck_html,each_deck_link,event,index)\n",
        "    decklist_add(deck_html,index, event)\n",
        "    this_deck = event[\"Decks\"][\"Deck \"+str(index)]\n",
        "\n",
        "  print(\"All Decks Processed\")\n",
        "  return event\n",
        "##################################################\n",
        "\n",
        "def compile_each_event(this_event_link):\n",
        "  #full_event_dictionary = compile_event_info(\"https://www.mtggoldfish.com/tournament/pauper-league-2022-07-27#paper\")\n",
        "  full_event_dictionary = compile_event_info(this_event_link)\n",
        "  return(full_event_dictionary)\n",
        "\n",
        "##################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing to Google Sheet"
      ],
      "metadata": {
        "id": "1q2qXwfWHaXJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section will process each of th relevant URLs in the returned JSON, then will call each of the relevant functions from the second block to process the infom from the URL, finally sending it back to the Google spreasheet API to be stored"
      ],
      "metadata": {
        "id": "KFCi7cZ2sXrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkAv9cEB5s4B"
      },
      "outputs": [],
      "source": [
        "from IPython.core.prefilter import EmacsHandler\n",
        "#Get the list of links from the stored file Website Check Dicitonary\n",
        "filepath = \"/content/drive/MyDrive/Colab Notebooks/Pauper Deck Analysis 3.0/Website Check Dictionary\"\n",
        "save_dictionary_writepath = \"/content/drive/MyDrive/Colab Notebooks/Pauper Deck Analysis 3.0/AWS Write File\"\n",
        "\n",
        "\n",
        "with open(filepath,'r') as readfile:\n",
        "  status_dictionary = readfile.read()\n",
        "  status_dictionary = json.loads(status_dictionary)\n",
        "\n",
        "print(len(status_dictionary))\n",
        "\n",
        "save_dictionary = {}\n",
        "for each_record in status_dictionary:\n",
        "  print(each_record)\n",
        "  current_record = status_dictionary[each_record]\n",
        "\n",
        "  #Sort through the ones where index [2] = \"Good\" and index[3] = \"not stored\"\n",
        "  if current_record[2] != \"Good\" or current_record == \"stored\":\n",
        "    status_dictionary.pop(each_record)\n",
        "  else:\n",
        "    event_dictionary = compile_each_event(current_record[1])\n",
        "    save_dictionary[each_record] = event_dictionary\n",
        "    print(event_dictionary)\n",
        "    with open(save_dictionary_writepath, 'w') as writefile:\n",
        "      writefile.write(json.dumps(save_dictionary))\n",
        "      writefile.close()\n",
        "\n",
        "#\n",
        "with open(save_dictionary_writepath,'r') as readfile:\n",
        "  print(json.loads(readfile.read()))\n",
        "\n",
        "\n",
        "#Need to store all the events in a new dictionary and the write them to a new file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(save_dictionary_writepath,'r') as readfile:\n",
        "  this_file = json.loads(readfile.read())\n",
        "  print(this_file)\n",
        "  print(len(this_file))"
      ],
      "metadata": {
        "id": "OwNHF_FVDUB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Re-write of Google Sheets Functionality"
      ],
      "metadata": {
        "id": "oC_vEm4MuaVQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of blocks is a re-writre of the google sheets sheet where we generate the URLs for the events and check the validity of those links"
      ],
      "metadata": {
        "id": "_b07BCFbbuqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if website is live by jsut downloading the header instead of the entire HTML\n",
        "#https://stackoverflow.com/questions/16778435/python-check-if-website-exists\n",
        "\n",
        "def check_site(url_to_check):\n",
        "  response = requests.get('http://www.example.com')\n",
        "  if response.status_code == 200:\n",
        "      return (\"Good\")\n",
        "  else:\n",
        "      return(\"Bad\")\n",
        "\n"
      ],
      "metadata": {
        "id": "OJjVokozatux"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block is used to iterate through old dates to see if a pauper league event exists. It checks the validity of these links using the HTML checker above."
      ],
      "metadata": {
        "id": "rv2aPIKR_6nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_period = \"2015-01-07\"\n",
        "#Convert to strptime in order to compare to today\n",
        "start_period = datetime.strptime(start_period,'%Y-%m-%d')\n",
        "current_date = datetime.today()\n",
        "event_dictionary = {}\n",
        "filepath = \"/content/drive/MyDrive/Colab Notebooks/Pauper Deck Analysis 3.0/Website Check Dictionary\"\n",
        "\n",
        "#open the readfile to see if it contains the current date\n",
        "with open(filepath,'r') as readfile:\n",
        "  readdictionary = readfile.read()\n",
        "\n",
        "#check if readdictionary is blank. If not blank, load the JSON. if yes blank, create a dictionary\n",
        "if readdictionary is not \"\":\n",
        "  readdictionary = json.loads(readdictionary)\n",
        "  #event_dictionary = readdictionary\n",
        "else:\n",
        "  readdictionary = {}\n",
        "\n",
        "#for each week leading up to this one\n",
        "while start_period < current_date:\n",
        "  #build the website url from the date\n",
        "  event_datecode = start_period.strftime('%Y-%m-%d')\n",
        "  event_URL = \"https://www.mtggoldfish.com/tournament/pauper-league-\"+event_datecode+\"#paper\"\n",
        "\n",
        "  #create a variable to track if the date has already been checked\n",
        "  if event_datecode in readdictionary.keys():\n",
        "    already_in_use = \"yes\"\n",
        "  else:\n",
        "    already_in_use = \"no\"\n",
        "\n",
        "  if already_in_use == \"no\":\n",
        "    site_status = check_site(event_URL)\n",
        "    event_code = \"Pauper League \"+event_datecode\n",
        "    storage_status = \"not stored\"\n",
        "    readdictionary[event_code] = [event_datecode,event_URL,site_status,storage_status]\n",
        "\n",
        "  #adding days to a date\n",
        "  #https://stackoverflow.com/questions/6871016/adding-days-to-a-date-in-python\n",
        "  end_date = start_period + timedelta(days=7)\n",
        "  start_period = end_date\n",
        "\n",
        "print(readdictionary)"
      ],
      "metadata": {
        "id": "FJHATX7hNTHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block saves the dictionary to the file on my google drive"
      ],
      "metadata": {
        "id": "zhWCNf_m7lds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filepath, 'w') as writefile:\n",
        "    writefile.write(json.dumps(readdictionary))\n",
        "    #writefile.write(\"\")\n",
        "    writefile.close()\n",
        "\n",
        "\n",
        "with open(filepath,'r') as readfile:\n",
        "    readdictionary = readfile.read()\n",
        "    readdictionary = json.loads(readdictionary)\n",
        "    print(readdictionary)\n",
        "    print(len(readdictionary))\n"
      ],
      "metadata": {
        "id": "UtpyKNfUNS-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test to see if removing a value from the file and dictionary will be fixed when I re-run the file."
      ],
      "metadata": {
        "id": "5V3UhY2-7dMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Default title text\n",
        "with open(filepath, 'w') as writefile:\n",
        "    key = \"Pauper League 2016-10-12\"\n",
        "    if key in readdictionary.keys():\n",
        "      readdictionary.pop(key)\n",
        "    print(readdictionary)\n",
        "    writefile.write(json.dumps(readdictionary))\n",
        "    #writefile.write(\"\")\n",
        "    writefile.close()\n",
        "#print(event_dictionary)\n"
      ],
      "metadata": {
        "id": "WaEyd07V_xev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to the data studio account:\n",
        "\n",
        "https://datastudio.google.com/u/0/reporting/7214b19c-cad6-44a8-9892-fbea5dc419b1/page/qWc0C/edit\n",
        "\n",
        "Link to the Googls Sheet:\n",
        "\n",
        "https://docs.google.com/spreadsheets/d/1d-X0qKTMXzWmUTLlmgjiHNuLPZdJ4CxrDiSdKQiQ_qE/edit#gid=771810328"
      ],
      "metadata": {
        "id": "-AElvkp4dUCM"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p2UhDnvGucPdzLpwlXJfeA1fYB23fKk3",
      "authorship_tag": "ABX9TyMf+gXNYwsmnb/crmAJm7FV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}